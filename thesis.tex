\documentclass[12pt, notitlepage]{article}
\usepackage[margin=4cm]{geometry}
\usepackage{hyperref}
\usepackage[english]{babel}
\usepackage{tocloft}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{float}
\usepackage{tabularx}
\usepackage[ruled,vlined]{algorithm2e}

\setcounter{tocdepth}{4}
\cftsetindents{paragraph}{1cm}{0cm}

\lstset{ %
  basicstyle=\footnotesize,        % the size of the fonts that are used for the code
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  breaklines=true,                 % sets automatic line breaking
  captionpos=b,                    % sets the caption-position to bottom
  deletekeywords={...},            % if you want to delete keywords from the given language
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  morekeywords={*,...},            % if you want to add more keywords to the set
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  tabsize=4,                       % sets default tabsize to 2 spaces
  title=\lstname                   % show the filename of files included with \lstinputlisting; also try caption instead of title
}


\title{Functional Testing of Web Applications}
\author{Stefan Gamerith\\\\
		\emph{Linzerstrasse 429 4215,}\\
		\emph{1140 Wien}\\
		\emph{Student ID: 0925081}}

\begin{document}
	\maketitle
	\thispagestyle{empty}
	\begin{abstract}
		Since Web applications are more and more replacing traditional Desktop applications making Web engineering one of the
		most important disciplines in the IT industry. Thus functional tests verifying the functioning of an application are
		crucial to the success of a Web project. This paper first gives an introduction to Software testing in general and
		outlines two different Web Test Automation concepts in particular. Although there exists some distinct approaches 
		to overcome the difficulties of regular changes in Web applications, non of them have been applied in context of 
		functional Web application testing.   
		
	\end{abstract}
	\newpage
	\onehalfspacing
	\tableofcontents
	\thispagestyle{empty}
	\singlespacing
\newpage
\setcounter{page}{1}

\section*{Introduction}
\addcontentsline{toc}{section}{Introduction}
Since the first proposal of HTTP\cite{http-proposal} the Web sites evolved from 
text pages implementing the Request/Response Pattern\cite{request-response}  to  
complex Web applications. Whereas the former draws a clear distinction between a client
who performs a request and a server who sends the response, the latter does not show this 
clear differentiation. Even recently a new communication protocol\cite{web-socket} has been published 
in which the browser acts like a server, thus listening for requests. Tilley and Huang \cite{web-classification} proposed
a new classification scheme for Web applications. They partition Web applications into three different categories:
Class 1 are mainly static Web applications containing static content. Usually these are implemented as plain HTML sites. 
Class 2 are Web applications with some sort of dynamic behaviour. Dynamic content is realized with technologies like Flash\cite{flash},
embedded Java Applet\cite{java-applet}, Javascript or recently CSS3\cite{css3}. 
Class 3 represents the most complex Web applications. In addition to Class 1 and 2 these provide dynamically generated content by server-side technologies
like JSP, PHP, ASP and others. Nowadays most Web applications represent Class 3. Unfortunately developing these requires much more expertise, increasing the
likelihood of defects and often requiring much higher maintenance costs.\\
Probably one of the most challenging aspects are constant changes. P.J.Warren, C.Boldyreff, M.Munro\cite{html-evolution}
presented a study which analyzed six websites. Their study shows a correlation between the complexity and the changes of
a website: The more complex a website get the more likely they will change in the future. 
There are structural and behavioural changes. While the former means changes in the document structure (e.g. Document Object Model\cite{dom} or HTML tree), the latter
includes all changes affecting how Web applications respond when an action is performed. 
This requires adaptive testing techniques. Due to the importance of this characteristic, a whole section in this thesis outlines the challenges faced
with changing environments.\\ 
Another important aspect is the huge user population. While in the beginnings of the World Wide Web only universities had access,
nowadays the vast majority of the population can view content distributed across all over the world. The typical Web user range from a
mother who does an online shopping tour over a teenager updating the current relationship status in Facebook to a student doing research for
the Bachelor thesis. The integration of designers understanding these different social backgrounds of users is crucial. All of the
above characteristics of Web applications make functional testing a difficult task though not impossible.\\\\
This thesis first gives an overview of what is actually meant by Software testing and then outlines the different testing approaches. 
The remainder of the first section discusses the three fundamental testing levels. In the second section general traps and pitfalls of Software Automation are outlined and two classes Test Automation Frameworks are presented and compared. The third section briefly introduces some basic approaches trying to overcome the difficulties of regular changes in Web applications. 
\newpage

\section{Software Testing}
G. Myers\cite{art-of-software-testing} define software testing as "the process of executing a program with the intent of finding errors".
According to that one might conclude that finding bugs is the only purpose of software testing. Since TDD\cite{tdd} became popular
another definition coexist: "Software testing is the process of verifying the programs output against predefined values". Besides from 
these IEEE\cite{ieee-definition} defines software testing as: "The process of operating a system or component under specified conditions, observing or recording the results, and making an evaluation of some aspect of the system or component".\\
\subsection{Testing Techniques}
This section outlines the different software testing techniques\cite{testing-methods} and briefly describes testing methods representing each of the mentioned techniques in context of Web application testing.
\paragraph{Black-Box Testing} ~\\
This testing technique looks at the code which needs to be tested as a whole. It requires no internal knowledge though test cases are typically 
derived from functional requirements. Therefore some sort of input data is verified against generated outcome.
It is impossible to test all possible input combinations. In \cite{softare-testing-principles} this is named as \textit{Dijkstra's Doctrine}
where for a program accepting a six-character code, ensuring that the first character is numeric and the rest are alphanumeric, an input 
sequence of 9161328320 combinations needs to be generated.
According to \cite{testing-overview} one representative implementing this approach is proposed by Di Lucca et al.\cite{decision-table-testing} which uses a decision table as it's essential part.  
\paragraph{White-Box Testing} ~\\
In contrast this technique examines the internals such as code, code structure and control flow. White-Box Testing can be further classified in
static and structural testing. The former does not require executing the code thus the source code is sufficient for examination. The source code
is analyzed either by humans through a code review or by static analysis tools which check for unreachable code, unused variables, memory leaks, use
deprecated libraries and other metrics. The latter executes the program and use the control structure for coverage examination\cite{structural-testing}.
\paragraph{Grey-Box Testing} ~\\
To clarify the naming conflict, the two terms Grey-Box Testing and Gray-Box Testing are used synonymously in the literature\cite{bridge-grey}. 
In the context of Web applications this new testing technique arises\cite{web-engineering}. Grey-Box Testing is a mixture of Black-Box and White-Box Testing as Grey-Box Testing (Black box + White box = Grey Box).\\
One candidate representing this testing technique is \textit{User-Session based testing}. Though the behaviour of Web application is examined like in
Black-Box Testing it requires some internal knowledge (e.g. links to other pages) due to it verifies page or link coverage.
\subsection{Testing Levels}
Testing Levels are no different than in traditional software applications. 
In this section three fundamental testing approaches, namely \textit{Unit Testing}, \textit{Integration Testing} and 
\textit{System Testing}, beginning from fine-grained to coarse-grained testing levels. 

\paragraph{Unit Testing} ~\\
IEEE defines Unit Testing as\cite{ieee-definition} "testing of individual units or groups of related units". In \cite{sw-testing-quality-assurance}
a unit "is the smallest possible testable software component". Another, yet more practical definition is\cite{practical-unit-testing-definition}:
A unit test is "a test, executed by the developer in a laboratory environment, that should demonstrate that the program meets the requirements set in the design specification". Though these definition clearly clarify what unit testing is in general, often there are misunderstandings. \cite{unit-testing-survey} conclude
before the specification of Unit tests a company should determine the granularity of units and what needs to be tested.
These questions help agreeing on Unit testing principles when developing in a team:\\
\begin{itemize}
	\item \textbf{How is a unit test conducted?}\\
	Nowadays most developers agree on conducting unit tests in the from of test-driven design(TDD)\cite{beck-tdd}. 
	In the earlier days there did not even exist code verifying other code. The only meta language to describe the functionality of a software component was either
	code comments or separated written English text.
	\item \textbf{When are the unit tests executed?}\\
	Unit should should have no external dependencies causing long running Unit tests. In TDD fashion non dependable tests are executed during each iteration,
	usually several times a day. Long running Integration tests typically run as nightly build ideally executed by an automated build environment like
	Maven\cite{maven} or Ant\cite{ant}.
	\item \textbf{Who decides how the unit test shall be conducted?}
	As mentioned above fine grained tests are executed by a developer during one single iteration. It is his/her own responsibility whether a chunk of code needs to
	be tested or not.
\end{itemize}
As we know what a Unit test is one important question arise: How to select Unit test cases? At one extreme test cases with one hundred percent statement coverage
are surely too much. On the other hand no testing at all lead to undesirable behaviour at least or even worst unrecoverable damage. 
As a rule of thumb only code causing state changes should be tested. For example testing a method returning only the state of an object or variable (e.g. getter/setter methods) is unnecessary, though they can be used in context with other computations. Unit testing frameworks 
like JUnit\cite{junit} offer a special method dedicated especially for setting up the environment.\\
Writing testable code affects the whole development process. It changes the design of classes fundamentally. One class should serve exactly one and only one purpose. Breaking this principle means high coupling, thus having a dependent class. Dependencies to other classes are not desirable because they introduce new
side effects. In addition third party dependencies may lead to unpredictable or erroneous behaviour because of blindly trusting foreign code.
As a best practice third party dependencies should be kept in one single place. This has the advantage of testing third party code in isolation 
and one single point of failure instead of many. However sometimes class level dependencies can not be removed.
One solution T. Mackinnon, S. Freeman and P. Craig proposed\cite{mock-objects} is using mock objects instead of real ones. In the literature the terms \textit{mock object} and \textit{stub implementation} are often used synonymously though M. Fowler clearly differentiate\cite{fowlermocks} (together with similar confusing terms): A \textit{Mock object} is a object having the same interface as the real implementation. It can be initialized with state for further verification. 
A \textit{Stubs} "provide canned answers to calls made during the test." It usually does provide some predefined behaviour.\\ 
It can be summarized that Unit testing is an important technique ensuring software quality. Due to the fine granularity of test cases they Unit tests 
should run in isolation. 

\paragraph{Integration Testing} ~\\
As Web applications consists of multiple components there is a need for strategies to combine them. Integration testing is defined as\cite{sw-testing-quality-assurance} "testing the interaction between the modules and interaction with other systems externally". IEEE defines
\textit{integration} as\cite{ieee-definition} "the process of combining software components, hardware components, or both into an overall system" and \textit{Integration testing} as\cite{ieee-definition} 
"Testing in which software components, hardware components, or both are combined and tested to evaluate the interaction between them".\\
Integration testing is different depending on the chosen integration strategy. The list below summarizes the main software integration strategies\cite{schattenbest}:\\
\begin{itemize}
	\item \textbf{Big-Bang integration}\\
	IEEE defines it as\cite{ieee-definition} "A type of integration [testing] in which software elements, hardware elements, or both are combined all at once into an overall system, rather than in stages". These strategy is the simplest and the most obvious though software system are rarely integrated following this approach. It assumes that all integrated software components work well together and all possibilities are taken into account from the beginning. This is of course
unrealistic in today's software applications as dozens of developers work on the same application having more and more dependencies to legacy code. 
	\item \textbf{Continuous integration}\\
	This is probably one of the most popular strategies nowadays. Even small software pieces are integrated continuously. Depending on the 
	size and complexity components are integrated daily (\textit{daily build}), weekly (\textit{weekly build}) or monthly (\textit{monthly build}). 
	Sooner or later as the project progresses one problem arises: What to do if a component can not be integrated because it depends on an unimplemented 
	piece of code? A solution would be to simulate the unimplemented component. This is similar to the concept of \textit{mock objects} mentioned earlier 
	except the object need to provide at least a minimal implementation or even better a fake implementation. If this is not possible another type of integration 
	strategy needs to be chosen locally even though the global continuous integration strategy remains. 
	\item \textbf{Top-down integration}\\
	Software components are integrated from top to bottom. A typical (Web) application consists of three or more layers. Integration starting at the topmost layer
	provides an overview of the whole system even early in the development process. The drawback of this strategy is the simulation of bottom layers with the risk
	of false expectations. Fortunately not all components depend on pieces of code in lower layers. These components are often called
	\textit{sub-system}\cite{sw-testing-quality-assurance}. Sub-systems work isolated from the rest of the components. 
	\item \textbf{Bottom-up integration}\\
	It is basically the opposite of the above. Components residing at the lower layer are integrated before those at higher layers. 
	It doe not require any simulation because all components are defined from ground up. Users interact with an application through the graphical user
	interface residing at the topmost layer which will be implemented late, thus users can interact with the application at the end of the development phase. 
	We recommend developing a demo application (e.g. \textit{prototype}) providing a first impression of the whole system.
	\item \textbf{Bi-directional integration}\cite{sw-testing-quality-assurance}\\
	This approach, also known as \textit{Build integration}\cite{schattenbest}, combines both Top-down and Bottom-up integration. 
	Individual components are grouped into categories. First components in each category are integrated bottom-up leaving the topmost layer.
	Then all remaining components are integrated top-down.
\end{itemize}
\paragraph{System Testing} ~\\
IEEE defines \textit{System testing} as\cite{ieee-definition} "testing conducted on a complete, integrated system to evaluate the system's compliance with its specified requirements". As the whole application is finished System testing begins. It aims as evaluating the system against initial requirements.
Depending on the type of requirement either test cases implementing \textit{functional} or \textit{non-functional} requirements are executed as they are defined during the requirement-gathering phase. 
\begin{itemize}
	\item \textbf{Functional testing}\\
	IEEE's defines Functional testing as \cite{ieee-definition}
	"Testing conducted to evaluate the compliance of a system or component with specified functional requirements". Thus functional tests ensure
	the correct functioning or behavior of the whole system and they are typically of type black-box.
	They focus on evaluating the outcome against predefined expectations. Functional testing of Web applications is hard due to the
	reasons mentioned in the introduction. This is the reason why there is no tool ultimate tool satisfying all needs and no best practice, though are many
	tools implementing different approaches.
	\item \textbf{Non-functional testing}\\
	There are several approaches implementing this kind of testing. One are performance tests. Since the well known publication of Gordon E. Moore back in 
	1965\cite{moore} which says that the number of integrated circuits will at least double every year computation power indeed highly increased.
	However performance tests are still important due to the increasing complexity and feature richness of todayâ€™s applications. Another non-functional 
	testing approach is security testing. Though the Open Web Application Security Project, abbreviated OWASP\cite{owasp}, published the list of the top ten
	Web security threats, the number of exploits listed in the Metasploit Database\cite{metasploit} still rises. Thus security testing is crucial for the success
	of a software company. Apart from the above there are other approaches which would exceed the scope of this thesis. 
\end{itemize}
\section{Test Automation}
Most of the time developers spend time reading other's code. In the remaining time they write code, chat with colleagues or do other things.
Therefore time consuming log running tasks should be done manually by someone else or automated.
While the former might seem outdated in some areas there are still manual testing techniques which can not be replaced by automation.
One the other hand test automation tools gain increasing importance.\\
This section first compares the advantages and disadvantages of test automation and compares the different kinds of test automation frameworks.
\subsection{Pitfalls of Test Automation}
Possible pitfalls and misconceptions of test automation are listed below\cite{test-automation-success}.
\begin{itemize}
	\item \textbf{Automated testing completely replaces manual testing.}\\
	Often people who do not understand testing or even testers and test managers who should agree on this statement. Another related misconception 
	is to automate all tests. B. Marick\cite{testing-mistakes} differences 2 economic facts regarding automation versus manual testing:
	"If an automated test costs a certain amount of money, it will cost almost nothing to run from then on." and
	"If a manual test costs a certain amount of money to run the first time, it will cost just about the same as first time to run each time thereafter."
	These two statements conclude that complex automated testing should be done manually for economic reasons.
	However \cite{test-automation-success} lists some guidelines whether tests should be automated or not: Tests running on every build,
	testing on many different platforms and the same setup procedures for a lot of different tests. \\
	Another, yet more significant, reason is that the only way the evaluate automated tests by manual reviews. This is a variant of the well known
	chicken egg problem. In order to test the quality of automated tests meta test cases are required which again require additional test cases and so on.
	\item \textbf{The Pesticide Paradox}\\
	S. Desikan and G. Ramesh\cite{softare-testing-principles} compare writing software test cases with "designing the right pesticides to catch and kill the pests".
	It means that developers writing code get used to automated test cases and develop \textit{just for} passing the test but test cases never cover one hundred 
	percent of all possible execution paths. 
	\item \textbf{Independence}\\
	Probably every developer knows that Unit test should run independent from each other. Fortunately these tests usually have low coupling, requiring minimal
	overhead to set up. But \textit{all} kinds of tests should run in isolation, thus not affecting the outcome of other tests.
	This is extremely challenging especially for coarse grained tests like functional Web application tests. However some test cases logically depend on each other.
	We chose a simple Website offering online shopping functionality. One test case verifies the correct authentication behaviour for an arbitrary user and
	another evaluates the correct computation of the total price if a customer adds items to the shopping cart.
	Clearly the latter is only possible after successful authentication, thus the result of the former test case determines the result of the latter.
	Nevertheless testing needs to done independently. Therefore we recommend an isolated test environment as close as possible the production environment
	as a best practice.
	\item \textbf{Repeatability}\\
	At least as important as the above aspect is if tests executed repeatedly produce the same outcome in a stable environment. One possible 
	indicator of different outcomes are misused threading. As threads can lead to unpredictable behavior they should be avoided where possible. 
	If this is undesirable synchronization techniques help to reduce the risk of deadlocks and race conditions. 
	Another cause may be misused random data. If some application requires initialization with random data like playing dice, either using constant data
	or pre-generating the random data helps to predict the outcome. 
\end{itemize}
Apart from the above challenges automated testing is a great technique to make testing explicit. If new developers write code they immediately recognize 
the requirements formulated in test cases. Of course this is not true if management does not acquire enough resources or it is not treated as development activity.
Testing is a full time job and it should be done by dedicated experts.
Another important aspect ensuring successful automated tests is documentation in case of a more complex testing environment. Useful might be for example
documenting steps necessary for running the test cases, manual test result review in case of partial automated tests and what part of the software is not
tested and why. 
\subsection{Test Automation Frameworks}
As the World Wide Web attracted more and more people there has been a growing need for automated evaluation of Web applications. Companies are willing to
invest a lot for a powerful automation tool. All the above approaches have the term \textit{framework} in common. R. Johnson and B. Foote define it
in the context of Object Oriented Programming as\cite{oop}
"a set of classes that embodies an abstract design for solutions to a family of related problems, and supports reuses at a larger granularity than classes".
This definition can also be applied to automated testing where the term \textit{Test Automation Framework} seems appropriate.\\
E. Kit\cite{kit} distinguishes three generations of automation tools:
\begin{enumerate}
	\item The most basic tools use the \textit{Capture and Playback} approach. It works as a one needs to manually perform all steps necessary for evaluation which
	are subsequently recorded for replaying, hence the word \textit{replay} is often used synonymously.
	These tools have one major drawback in common as high maintenance costs\cite{record-playback}. Once everything is recorded there is no guarantee that all
	test cases do not break in the future though all work now. Another yet more problematic disadvantage are hard-coded data in the tool generated
	scripting language. This should be avoided in any programming language\cite{automation-principles}. 
	The last problem during playback is synchronization issues due to the distributed nature of Web applications. Obviously it takes some time for the browser to
	fetch a Website. In order to function correctly the tool implementing capture and playback needs to properly guess the delay after the server has sent a Website.
	Of course no tool is able to predict the future thus (over-)estimating the delay or requiring the programmer to explicitly put synchronization points
	into the generated script. Both solutions are cumbersome and hard.
	\item Second generation Capture and Playback tools have lessons learned. Instead of a single capture phase tools require writing scripting code. Vendors 
	equip their tools with a full blown scripted language which brings all the advantages of a modern programming language like code-reuse, conditional execution and
	error recovery to name a few. However treating automation as a software engineering task requires additional domain experts having scripting knowledge.
	\item Tools from the above two generations generate a single script for all test cases. Third generation Capture and Playback tools use many small scripts each 
	responsible for a single test case. It further extracts hard-coded data out of the scripts leading to a new era of frameworks called
	\textit{Data-driven Frameworks}. 
\end{enumerate} 
\paragraph{Data-driven Frameworks} ~\\
The basic concept of Data-driven Automation Frameworks is a separation between the behavioural instructions and the data used as parameter for each test case. 
As \cite{record-playback} stated parameters are usually kept in separated files organized as a grid. There is no limitation about the actual data format though
for usability reasons using spreadsheet readable formats is recommended as xsl-files. Even though the concept of Data-driven Automation Frameworks is not application specific until today only tools testing GUI and Web applications can be found. In the remainder of this paragraph a popular Data-driven Automation
tool namely \textit{Selenium}\cite{selenium} specifically designed for Web application testing is presented.\\
Selenium was originally developed by Jason R. Huggins working on a new time tracking system for ThoughtWorks back in 2004\cite{shortcut-selenium}.
At that time client-side Javascript code was a nightmare to test due to the different browser implementations. Almost a decade later it is still one of 
the most challenging tasks. Basically Selenium includes two different open source tools: \textit{Selenium IDE} and \textit{Selenium 2}. 
The former provide testing novices the ability to get used to how tests are recorded and the scripting environment. It currently runs on Mozilla Firefox only thus
shipping as a Firefox extension. The recommended way of using Selenium is through the latter tool which is the successor of Selenium 1. Developers have spent a lot of effort in backward compatibility, however is it of advantage to upgrade old Selenium 1 tests. \\
As Selenium mimics human user interaction, commands are \textit{the} essential part of interaction. All available commands can be
grouped into 3 command-groups: \textit{Actions}, \textit{Accessors} and \textit{Assertions}. While Actions typically alter the state of the application, Accessors operate on the state of the application and Assertions verify the application state. 
In addition Selenium provides \textit{Element Locators} and \textit{Patterns}. The former, as the name implies, tells Selenium where
to find HTML elements. The latter which is typically a regular expression can be used for instance of to identity the expected value 
of a HTML element.
\\\\ 
Table \ref{tab:selenium-commands} provides a short summary of the most important commands.\\ 

\begin{table}[H]
	\begin{tabularx}{\textwidth}{l|X|X}
		\textbf{Action} & \textbf{Selenium IDE Command\cite{ide-commands}} & \textbf{Selenium 2 Command\footnotemark\cite{selenium2-commands}} \\
		\hline
		Navigation to a specific URL & \texttt{open(URL)} & \texttt{driver.get(URL)}\\
		clicking on an input element & \texttt{click(locator)} & \texttt{*.click()}\footnotemark\\
		locating UI elements & \texttt{dom, xpath, css, identifier}\footnotemark & \texttt{driver.findElements(By.*)}\footnotemark\\
	\end{tabularx}
	\caption{A list of basic Selenium commands}
	\label{tab:selenium-commands}
\end{table}

\footnotetext[1]{All commands are examples written in the Java programming language}
\footnotetext[2]{* refers to any clickable HTML object}
\footnotetext[3]{An element locator is used as a parameter of various Action-commands. Locators usually have the format of
\textit{locatorType=argument}. Further information on element locators can be found on \cite{ide-commands}.}
\footnotetext[4]{If exactly one element is expected the singular form \texttt{driver.findElement} should be used, otherwise the 
plural form \texttt{driver.findElements} is suggested. For more information on element locators refer to \cite{selenium2-commands}}

One major difference between Web applications and traditional GUI applications is that the testing process and the actual execution process run asynchronously. Therefore failing test cases do not necessarily mean that a test case indeed fail. For that reason 
Selenium introduced \textit{"AndWait"} commands. Commands having the suffix AndWait perform it's intended action as it's
non-waiting counterparts, but in addition wait for the page to load after the action has been done. As in AJAX Web applications there
is no page refresh, \textit{"WaitForCondition"} commands provide an alternative to the above commands. A typical representative of
this groups of commands is the \textit{waitForElementPresent} command which stops executing until the specified element is present.
\\
Due to the following major drawback the use of tools following a Data-driven approach is not recommended for commercial applications. The reason for this
is scalability. To understand this problem the internals of a tool following a Data-driven approach need to be examined. Each row in a data-grid represents exactly 
a single test case. The implementation of the latter is done either automatically by recording user interactions or specified manually.
In either way as more applications code needs to be tested new test cases generated in one of the above ways are required. 
This leads to the conclusion that the number of lines of automation code is \textit{proportional} to the number of tests\cite{record-playback} which is generally 
undesirable. 
\paragraph{Model-driven Frameworks} ~\\
Model-driven Frameworks take a fundamentally different approach. They verify the correctness of a system using a technique called \textit{model checking}.
E. M. Clarke \textit{et al.}\cite{model-checking} defines model checking as "an automatic technique for verifying finite state [concurrent] systems".
The term \textit{finite state} typically refers to the concept of Finite State Machines(FSM)\cite{finite-state-machines} which is graphically represented by a directed graph where nodes make up the states. Changes from one state to another are called transitions. A FSM must have by definition exactly one start (entry) state and one or more end states.  In Model Checking a finite state model \textit{M} is limited by a constraint expressed as a logical formula \textit{f} where
following condition is checked: \textit{M} satisfies \textit{f}. A logical formula \textit{f} is composed of atomic propositions and booleans
expressions\cite{model-checking}. Researchers have spent a lot of effort in several temporal logic formalisms. The two most important ones are
\textit{computation tree logic(CTL)} and \textit{linear temporal logic} though for the purpose of model checking in the context of Model-driven Frameworks simpler
ones are sufficient\cite{dynamic-testing-web-applications}. \\
F. Ricca and P. Tonella proposed in their paper\cite{web-whiteBox-testing} one of the first model checking approaches. They introduced a
meta model (\textit{navigational model}) describing the general Web application structure which is given in the Unified Modeling Language (UML)\cite{uml}.
A navigational model shows the connections (\textit{links}) between Webpages, navigation constraints realized as key-value pairs and static and dynamic Webpages. 
Whereas the content of the former never changes, the content of the latter is computed dynamically at runtime using input parameters. Then a test case is
"a sequence of pages to be visited plus the input values to be provided to pages containing forms"\cite{web-whiteBox-testing}. Although the primary research focus
lies on \textit{path coverage} (e.g. verifying that a path is visited), their approach promises further testing criteria: All-paths testing, All-uses testing,
Hyperlink testing, Page testing and Defintion-use testing. As automatic test case generation scheme an algorithm is proposed which generates test cases for the above mentioned criteria, though it requires manual involvement for path coverage. Due to the latter factor and the growing number of rich client applications having a
single dynamic Webpage, more dynamic approaches have been explored.\\
H. Tanida et al.\cite{dynamic-testing-web-applications} extended the above approach to AJAX Web applications. In order to build a navigational model all 
application states need to be explored. Although they used the tool \textsc{Crawlajax}\cite{crawlajax} for exploring applications states, manual
crawling (\textit{guided crawling}) is still necessary to explore all application states. Automatic model generation is a time consuming and inefficient task. 
Therefore different approaches such as hashcode computation, state compression, recursive indexing\cite{recursive-indexing}, delta update or Sweep
Line\cite{sweep-line} have been taken into account in order to reduce state space\cite{state-space-reduction}. However \cite{dynamic-testing-web-applications}
implemented a \textit{state abstraction} technique which simplifies removing all redundant states. A tool
named \textsc{Goliath} accepts the generated navigation model and evaluates a set of expressions. Hence it operates on the DOM, it accepts XPath expressions like \texttt{doc.xpath('//a[@id="login"]').any?}. \\
Although Testing Ajax applications through model checking seems most promising, A. Deursen and A. Mesbah\cite{research-issues-model-checking-ajax} explored
a number of research questions, though their proposed model checker is quite different. Their model checker\cite{atusa} is based on \textit{invariants} which are
different constraints concerning the user interface. However a number of open research questions remain:
\begin{itemize}
	\item What is the most promising approach for expressing user interface constraints?
	\item Is it possible to construct a model of \textit{every} Ajax application?
	\item Hence Javascript code is a script language, how does Javascript faults affect the model generation process?
\end{itemize}
\vspace{1.5cm}
All in all, as in \cite{why-model-based-automation} Model based Testing Frameworks are the newest generation of Test
Automation Frameworks, hence it can be categorized as the \textit{fourth generation}. While the first and second generation, manual
testing and test scripting, mainly focuses on test execution neglecting test design, the third generation, capture-playback approach, separate test case definition from test-scripting. Although researches have investigated in extending the traditional capture-playback approach creating action-based, keyword-based, object-based or class-based approaches, every single one does require
some sort of human interaction. Though, as motivated in \cite{crawlajax}, fully automated model generation is not possible mainly
due to DOM state changes during script execution. 
 

\section{Resilience and Adaptiveness}
One of the main challenges in Web application testing are changing Webpages. There are even categories of Websites which were build with the intention of regular
changes such as News sites. Whereas on the fronted side news seem to be updates in real time, news sites are usually realized using a Content Management System. 
CMS systems ease the development of high adaptive Web applications, though requiring more sophisticated testing approaches. This section gives a brief overview 
of different approaches satisfying this requirement. 
\subsection{Robust XPath}
M. Abe and M. Hori proposed an approach\cite{robust-xpath} of stable XPath\cite{xpath} expressions, resisting structural changes.
XPath expressions ease the navigation through hierarchical content such as XML\cite{xml}, (X)HTML\cite{html}, DOM\cite{dom} and any other tree-like structure.
XPath defines several navigation axes though they all can be categorized as forward axes and reverse axes. While the former navigates downwards,
the latter navigates upwards. Web applications test tools use XPath expressions for the identification of nodes. While in principle these tools can deal with HTML
nodes and DOM nodes, we recommend using tools working with the latter thus offering the possibility to navigate through dynamic content. 
However tools often produce XPath expressions which break after slightly changing document structure. \\
M. Abe and M. Hori identified three types of XPath expressions: \textit{single-node pointing}, \textit{alternative predicate} and \textit{relative addressing}. 
Single-node pointing expressions identify, as the name suggests, exactly one single node. It will not point to any other nodes, Alternative predicate expressions
use a predicate expression to limit the number of result nodes. They do not necessarily have to return a single node, instead returning a set of nodes.
Relative addressing expressions use an fixed anchor node which will not change frequently.\\
An example of a simple html code is given in listing  \ref{lst:html}.
\begin{lstlisting}[language=HTML, caption={HTML code listing used as source for XPath expressions},label=lst:html]
<html>
	<body>
		<div>
			<label>Audi</label>
			<a href="http://www.audi.at"></a>
			<div>Perfect!</div>
		</div>
		<div>
			<label>VW</label>
			<a href="http://www.volkswagen.at"></a>
			<div>Perfect!</div>
		</div>
		<div>
			<label>Seat</label>
			<a href="http://www.seat.at"></a>
		</div>
	</body>
</html>
\end{lstlisting}
\vspace{1cm}
Table \ref{tab:xPath-expressions} shows an example for each type of XPath expression.
\begin{table}[H]
	\begin{tabularx}{\textwidth}{l|X}
		\textbf{Type} & \textbf{Expression} \\
		\hline
		single-node positioning & \texttt{/html[1]/body[1]/div[1]/label} \\
		single-node positioning & \texttt{/descendant::label[1]} \\
		alternative predicate & \texttt{//a[@href='http://www.volkswagen.at']} \\
		relative addressing & \texttt{//a[@href='http://www.volkswagen.at']/ ancestor::label[1]} \\
	\end{tabularx}
	\caption{Illustration of the three different types of XPath expressions}
	\label{tab:xPath-expressions}
\end{table}
The most promising approach is relative addressing if a link node is used an anchor because M. Abe and M. Hori observed that links are unlikely to change. 
Though some Websites heavily rely on numeric links as \texttt{http://orf.at/stories/2192632/} which change after a short period of time. Here URL rewriting 
is used to create human readable, bookmarkable links. Another problem are links with different link text though linking to the same page. Sometimes links are written with leading \texttt{http://} and other times not. However a slightly modified XPath expression solves the problem, e.g.
\texttt{//a[contains(./@href, www.volkswagen.at)]} instead of \texttt{//a[@href='http://www.volkswagen.at']}.\\
Robust XPath expressions are simple, yet effective surviving structural changes, however the authors noted that their empirical study was to small and the XPath expressions were tightly coupled to the provided HTML document in order to generalize their conclusions to all kinds of HTML documents.  
\subsection{Tree Similarity}
Until recently there has been spent a lot of effort in algorithms all addressing Tree Similarity due to the different areas of application as string similarity for spell checkers,
XML document analysis and text search algorithms to name just a few.
\paragraph{Tree edit distance} ~\\
Back in 1977 S. Selkow presented in his paper\cite{tree-editing-distance} an approach of measuring tree similarity. He observed that a tree can be transformed into another, similar tree 
using three operations:
\begin{itemize}
	\item \textbf{Insert}\\
	Insert a new child node at a certain position, substitute former child nodes with the node to be inserted and insert former child nodes as children of the new inserted node.  
	\item \textbf{Delete}\\
	Remove a node from a tree and connect it's children directly to it's parent node.
	\item \textbf{Rename(Update)}\\
	Change the label of a node preserving the overall structure of the tree.
\end{itemize}
The above informal definition implies that insert and delete are inverse edit operations (i.e., insert undoes delete and vice versa). Figure \ref{fig:tree_operations} illustrates all three operations (adapted from \cite{tree-edit-distance-survey}). 
\begin{figure}[H]
        \centering
        \begin{subfigure}[h!]{0.3\textwidth}
                \includegraphics[width=\textwidth]{tree_edit_distance1}
                \caption{Source tree for all operations}
        \end{subfigure}
\\
        \begin{subfigure}[h!]{0.3\textwidth}
                \includegraphics[width=\textwidth]{tree_edit_distance2}
                \caption{Tree after deletion of node \textit{c}}
        \end{subfigure}
\quad
        \begin{subfigure}[h!]{0.3\textwidth}
                \includegraphics[width=\textwidth]{tree_edit_distance3}
                \caption{Tree after insertion of node \textit{g}}
        \end{subfigure}
\quad
        \begin{subfigure}[h!]{0.3\textwidth}
                \includegraphics[width=\textwidth]{tree_edit_distance4}
                \caption{Tree after relabelling \textit{c} to \textit{a} and \textit{d} to \textit{f}}
        \end{subfigure}
        \caption{Illustration of tree-edit operations}\label{fig:tree_operations}
\end{figure}
The tree edit distance, formally defined as $\delta(T_1,T_2)$ whereas $T_1$ and $T_2$ are labelled trees,
is the minimal number of operations necessary for transforming $T_1$ into $T_2$: min\{$\gamma(S)|S$ is a sequence of operations transforming $T_1$ into $T_2$\}. $\gamma(S)$ is a function measuring the cost of a sequence of operations $S=s_1,...,s_k$. \\
Although a large number of algorithms have been developed in the past (e.g. Tai\cite{tai} introduced the first algorithm with non exponential time complexity), there are faster, yet more complex solutions to the domain of tree similarity. 

\paragraph{Longest common subsequence} ~\\
Instead of viewing tree similarity as a sequence of operations, these paragraph introduces algorithms which are based on finding
the the longest common subsequence of linked nodes. 
Though Yang\cite{yang} introduced an algorithm addressing the similarity between two code fragments, H. Wang and Y. Zhang\cite{simple-tree-matching} followed a similar approach in context of Web data extraction.\\
Algorithm \ref{lst:simple-tree} shows the function \textit{SimpleTreeMatching(A,B)} which takes two trees \textit{A},\textit{B} and returns the number of the longest continuous sequence of connected nodes.\\ 
\\
\begin{algorithm}[H]
\SetAlgoLined
	\If{the roots of the two trees $A$ and $B$ contain distinct symbols}{
		\Return{0};
	}
	$m$ := the number of first-level subtrees of $A$;\\
	$n$ := the number of first-level subtrees of $B$;\\
	$M[i,0]$ := 0 for $i=0,\ldots,m$;\\
	$M[0,j]$ := 0 for $j=0,\ldots,n$;\\
	
	\For{$i:=1$ to $m$}{
		\For{$j:=1$ to $n$}{
			$M[i,j]$:=$max(M[i,j-1], M[i-1,j], M[i-1,j-1] + W[i,j])$
				where $W[i,j]$ = $SimpleTreeMatching(A,B)$\\
				where $A$ and $B$ are the i-th and j-th first-level subtrees of $A$ and $B$ respectively
		}
	}
	
	\Return{$M[m,n] + 1$};
\caption{SimpleTreeMatching(A,B)\label{lst:simple-tree}}
\end{algorithm}
\vspace{\baselineskip}
As \cite{yang} mentioned two nodes are considered identical if and only if they contain identical symbols. For simplicity
node symbols are limited to letters of the English alphabet, however every other content is imaginable, for instance HTML-tags
as illustrated in \cite{simple-tree-matching}. However a more appropriate equality criteria is tag \textit{and} attribute equality. 
In some cases the above criteria might be too restrictive, thus opening a new area area of research for extending the above algorithm.
One approach would be an extension taking attribute similarity and tag similarity into account as existing
ones\cite{simple-tree-matching} only consider the latter one.\\
Another drawback of the above algorithm is that two trees do not match even if they differ only in the root node. 
Therefore Yang\cite{yang} proposed an extension of the \textit{SimpleTreeMatching} algorithm loosening the constraint of matching
nodes. Two nodes match if they are \textit{comparable} or \textit{identical}: Every identical node is also comparable but not the other way round. \\
Due to the wide area of application a lot of effort has been spent into algorithms addressing tree similarity.
To discuss all aspects regarding tree similarity would go beyond the scope of this thesis. 

\subsection{Other Approaches}
There are plenty of other research work addressing different areas, though some ideas seem also appropriate for adaptive functional
Web application testing. This subsection outlines some of them and briefly discusses further research approaches using ideas from
below.\\
As Y. Yang and H. Zhang\cite{page-analysis_visual} motivated in their paper tools relying on the structural organization of a
Website are not feasible due to the common misuses of the HTML markup language. Thus the W3C Consortium suggest Cascade Style Sheets(CSS\cite{css3}) facilitating a clear separating between content and appearance. Event though the Document Object Model(DOM\cite{dom})
seems a better choice combining both in a consistent way, there are similar looking Website though differently organized.  
As a remedy to this L. Wenyin et al.\cite{phishing-visual} first proposed an approach measuring the visual similarity of a Website.
They measure visual similarity in three metrics: \textit{block level similarity}, \textit{layout similarity} and
\textit{style similarity}. To calculate these metrics a Webpage is divided into salient blocks. Whereas block level similarity is defined as "the weighted average of all pairs of matched blocks", layout similarity is defined as "the ratio of the weighted number
of matched blocks to the number of total blocks". The latter metric is can be calculated using the histogram of the style feature. 
A. Fu et al.\cite{emd} proposed a different approach measuring similarity based on visual cues. A Webpage is first converted into 
images bringing the advantage of structural independence meaning that their approach is not limited to HTML Webpages. Then the
\textit{Earth Mover's Distance}\cite{emd-def} is calculated providing a universal, comparable metric. \\
X. Gu et al.\cite{understanding-web-adaption} proposed a fundamentally different approach toward Web content adaption. 
Their paper is based on the knowledge of the objective of Web publishers inferring that Web authors first decides \textit{what}
to present and \textit{then} decides \textit{how} to present the information. Unlike former content adaption
technologies\cite{function-object-model, adaptive-html-delivery} an automatic top-down, tag-tree independent approach is presented
removing the limitation of a physical realization format. The essential part is the \textit{Web Content Structure} which is
formally defined as a triple $\Omega=(O,\Phi, \delta)$. Whereas $O$ is a finite set of identifiable objects, $\Phi$ describes
possible separators and $\delta$ defines the relationship of every object pair. In other words $\Omega$ is tag-tree independent 
abstraction of the visual representation of a Web page. In addition X. Gu et al.\cite{understanding-web-adaption} provide
an algorithm, automatically inferring the Web Content Structure from any Webpage. Therefore the detection process recursively 
divides a Webpage into smaller blocks and then merging similar ones via visual similarity. \\\\
To sum up, even though there are many different approaches addressing changing Webpages in isolation, every one has it's strength
and weaknesses. Yet, not a single approach have been applied to functional Web testing techniques offering wide area of research.

\section*{Conclusion}
\addcontentsline{toc}{section}{Conclusion}


\newpage
\bibliography{literature}
\bibliographystyle{plain}
\end{document}


