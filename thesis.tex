\documentclass[12pt, notitlepage]{article}
\usepackage[margin=4cm]{geometry}
\usepackage{hyperref}
\usepackage[english]{babel}
\usepackage{tocloft}
\usepackage{setspace}

\setcounter{tocdepth}{4}
\cftsetindents{paragraph}{1cm}{0cm}

\title{Functional Testing of Web Applications}
\author{Stefan Gamerith\\\\
		\emph{Linzerstrasse 429 4215,}\\
		\emph{1140 Wien}\\
		\emph{Student ID: 0925081}}

\begin{document}
	\maketitle
	\thispagestyle{empty}
	\begin{abstract}
		This needs to be done.
	\end{abstract}
	\newpage
	\thispagestyle{empty}
	\onehalfspacing
	\tableofcontents
	\singlespacing
\newpage
\setcounter{page}{1}

\section*{Introduction}
Since the first proposal of HTTP\cite{http-proposal} the Web sites evolved from 
text pages implementing the Request/Response Pattern\cite{request-response}  to  
complex Web applications. Whereas the former draws a clear distinction between a client
who performs a request and a server who sends the response, the latter does not show this 
clear differentiation. Even recently a new communication protocol\cite{web-socket} has been published 
in which the browser acts like a server, thus listening for requests. Tilley and Huang \cite{web-classification} proposed
a new classification scheme for Web applications. They partition Web applications into three different categories:
Class 1 are mainly static Web applications containing static content. Usually these are implemented as plain HTML sites. 
Class 2 are Web applications with some sort of dynamic behavior. Dynamic content is realized witch technologies like Flash\cite{flash},
embedded Java Applet\cite{java-applet}, Javascript or recently CSS3\cite{css3}. 
Class 3 represents the most complex Web applications. In addition to Class 1 and 2 these provide dynamically generated content by server-side technologies
like JSP, PHP, ASP and others. Nowadays most Web applications represent Class 3. Unfortunately developing these requires much more expertise, increasing the
likelihood of defects and often requiring much higher maintenance costs.\\
Probably one of the most challenging aspects are constant changes. P.J.Warren, C.Boldyreff, M.Munro\cite{html-evolution}
presented a study which analyzed six websites. Their study shows a correlation between the complexity and the changes of
a website: The more complex a website get the more likely they will change in the future. 
There are structural and behavioral changes. While the former means changes in the document structure (e.g. Document Object Model\cite{dom} or HTML tree), the latter
includes all changes affecting how Web applications response when an action is performed. 
This requires adaptive testing techniques. Due to the importance of this characteristic a whole section in this thesis outlines the challenges faced
with changing environments.\\ 
Another important aspect is the huge user population. While in the beginnings of the World Wide Web only universities had access,
nowadays the vast majority of the population can view content distributed across all over the world. The typical Web user range from a
mother who does an online shopping tour over a teenager updating the current relationship status in Facebook to a student doing research for
the Bachelor thesis. The integration of designers understanding these different social backgrounds of users is crucial.\\


All of the above characteristics of Web applications make functional testing a difficult task though not impossible.
This thesis first gives an overview of the different testing approaches. For each 
testing method a tool implementing it will be presented.\\
-TODO Brief outline of the contents-
\newpage


\section{Software Testing}
\cite{art-of-software-testing} defines software testing as "the process of executing a program with the intent of finding errors".
According to that one might conclude that finding bugs is the only purpose of software testing. Since TDD\cite{tdd} became popular
another definition coexist: "Software testing is the process of verifying the programs output against predefined values". Besides from 
these IEEE\cite{ieee-definition} defines software testing as: "The process of operating a system or component under specified conditions, observing or recording the results, and making an evaluation of some aspect of the system or component".\\
\subsection{Testing Techniques}
This section outlines the different software testing techniques\cite{testing-methods} and briefly describes testing methods representing each of
the mentioned techniques in context of Web application testing.
\paragraph{Black-Box Testing} ~\\
This testing technique looks at the code which needs to be tested as a whole. It requires no internal knowledge though test cases are typically 
derived from functional requirements. Therefore some sort of input data is verified against generated outcome.
It is impossible to test all possible input combinations. In \cite{softare-testing-principles} this is named as \textit{Dijkstra's Doctrine}
where for a program accepting a six-character code, ensuring that the first character is numeric and the rest are alphanumeric, an input 
sequence of 9161328320 combinations needs to be generated.
According to \cite{testing-overview} one representative implementing this approach proposed by Di Lucca et al.\cite{decision-table-testing} will be described below.\\
-TODO detail description and explanation of \cite{decision-table-testing}-
\paragraph{White-Box Testing} ~\\
In contrast this technique examines the internals such as code, code structure and control flow. White-Box Testing can be further classified in
static and structural testing. The former does not require executing the code thus the source code is sufficient for examination. The source code
is analyzed either by humans through a code review or by static analysis tools which check for unreachable code, unused variables, memory leaks, use
deprecated libraries and other metrics. The latter executes the program and use the control structure for coverage examination\cite{structural-testing}.\\
-TODO detail description and explanation of \cite{web-whiteBox-testing}-
\paragraph{Grey-Box Testing} ~\\
To clarify the naming conflict, the two terms Grey-Box Testing and Gray-Box Testing are used synonymously in the literature\cite{bridge-grey}. 
In the context of Web applications this new testing technique arises\cite{web-engineering}. Grey-Box Testing is a mixture of Black-Box and White-Box Testing as Grey-Box Testing (Black box + White box = Grey Box).\\
One candidate representing this testing technique is \textit{User-Session based testing}. Though the behavior of Web application is examined like in
Black-Box Testing it requires some internal knowledge (e.g. links to other pages) due to it verifies page or link coverage.\\
-TODO detail description and explanation of \cite{user-session}-
\subsection{Testing Levels}
Testing Levels no different than in traditional software applications. TODO
TODO see \cite{sw-testing-quality-assurance}
\paragraph{Unit Testing} ~\\
IEEE defines Unit Testing as\cite{ieee-definition} "testing of individual units or groups of related units". In \cite{sw-testing-quality-assurance}
a unit "is the smallest possible testable software component". Another, yet more practical definition is\cite{practical-unit-testing-definition}:
A unit test is "a test, executed by the developer in a laboratory environment, that should demonstrate that the program meets the requirements set in the design specification". Though these definition clearly clarify what unit testing is in general, often there are misunderstandings. \cite{unit-testing-survey} conclude
before the specification of Unit tests a company should determine the granularity of units and what needs to be tested.
These questions help agreeing on Unit testing principles when developing in a team:\\
\begin{itemize}
	\item \textbf{How is a unit test conducted?}\\
	Nowadays most developers agree on conducting unit tests in the from of test-driven design(TDD)\cite{beck-tdd}. 
	In the earlier days there did not even exist code verifying other code. The only meta language to describe the functionality of a software component was either
	code comments or separated written English text.
	\item \textbf{When are the unit tests executed?}\\
	Unit should should have no external dependencies causing long running Unit tests. In TDD fashion non dependable tests are executed during each iteration,
	usually several times a day. Long running Integration tests typically run as nightly build ideally executed by an automated build environment like
	Maven\cite{maven} or Ant\cite{ant}.
	\item \textbf{Who decides how the unit test shall be conducted?}
	As mentioned above fine grained tests are executed by a developer during one single iteration. It is his/her own responsibility whether a chunk of code needs to
	be tested or not.
\end{itemize}
As we know what a Unit test is one important question arise: How to select Unit test cases? At one extreme test cases with one hundred percent statement coverage
are surely too much. On the other hand no testing at all lead to undesirable behavior at least or even worst unrecoverable damage. 
As a rule of thumb only code causing state changes should be tested. For example testing a method returning only the state of an object or variable (e.g. getter/setter methods) is unnecessary, though they can be used in context with other computations. Unit testing frameworks 
like JUnit\cite{junit} offer a special method dedicated especially for setting up the environment.\\
Writing testable code affects the whole development process. It changes the design of classes fundamentally. One class should serve exactly one and only one purpose. Breaking this principle means high coupling, thus having a dependent class. Dependencies to other classes are not desirable because they introduce new
side effects. In addition third party dependencies may lead to unpredictable or erroneous behavior because of blindly trusting foreign code.
As a best practice third party dependencies should be kept in one single place. This has the advantage of testing third party code in isolation 
and one single point of failure instead of many. However sometimes class level dependencies can not be removed.
One solution T. Mackinnon, S. Freeman and P. Craig proposed\cite{mock-objects} is using mock objects instead of real ones. In the literature the terms \textit{mock object} and \textit{stub implementation} are often used synonymously though M. Fowler clearly differentiate\cite{fowlermocks} (together with similar confusing terms): A \textit{Mock object} is a object having the same interface as the real implementation. It can be initialized with state for further verification. 
A \textit{Stubs} "provide canned answers to calls made during the test." It usually does provide some predefined behavior.\\ 
It can be summarized that Unit testing is an important technique ensuring software quality. Due to the fine granularity of test cases they Unit tests 
should run in isolation. 

\paragraph{Integration Testing} ~\\
As Web applications consists of multiple components there is a need for strategies to combine them. Integration testing is defined as\cite{sw-testing-quality-assurance} "testing the interaction between the modules and interaction with other systems externally". IEEE defines
\textit{integration} as\cite{ieee-definition} "the process of combining software components, hardware components, or both into an overall system" and \textit{Integration testing} as\cite{ieee-definition} 
"Testing in which software components, hardware components, or both are combined and tested to evaluate the interaction between them".\\
Integration testing is different depending on the chosen integration strategy. The list below summarizes the main software integration strategies\cite{schattenbest}:\\
\begin{itemize}
	\item \textbf{Big-Bang integration}\\
	IEEE defines it as\cite{ieee-definition} "A type of integration [testing] in which software elements, hardware elements, or both are combined all at once into an overall system, rather than in stages". These strategy is the simplest and the most obvious though software system are rarely integrated following this approach. It assumes that all integrated software components work well together and all possibilities are taken into account from the beginning. This is of course
unrealistic in today's software applications as dozens of developers work on the same application having more and more dependencies to legacy code. 
	\item \textbf{Continuous integration}\\
	This is probably one of the most popular strategies nowadays. Even small software pieces are integrated continuously. Depending on the 
	size and complexity components are integrated daily (\textit{daily build}), weekly (\textit{weekly build}) or monthly (\textit{monthly build}). 
	Sooner or later as the project progresses one problem arises: What to do if a component can not be integrated because it depends on an unimplemented 
	piece of code? A solution would be to simulate the unimplemented component. This is similar to the concept of \textit{mock objects} mentioned earlier 
	except the object need to provide at least a minimal implementation or even better a fake implementation. If this is not possible another type of integration 
	strategy needs to be chosen locally even though the global continuous integration strategy remains. 
	\item \textbf{Top-down integration}\\
	Software components are integrated from top to bottom. A typical (Web) application consists of three or more layers. Integration starting at the topmost layer
	provides an overview of the whole system even early in the development process. The drawback of this strategy is the simulation of bottom layers with the risk
	of false expectations. Fortunately not all components depend on pieces of code in lower layers. These components are often called
	\textit{sub-system}\cite{sw-testing-quality-assurance}. Sub-systems work isolated from the rest of the components. 
	\item \textbf{Bottom-up integration}\\
	It is basically the opposite of the above. Components residing at the lower layer are integrated before those at higher layers. 
	It doe not require any simulation because all components are defined from ground up. Users interact with an application through the graphical user
	interface residing at the topmost layer which will be implemented late, thus users can interact with the application at the end of the development phase. 
	We recommend developing a demo application (e.g. \textit{prototype}) providing a first impression of the whole system.
	\item \textbf{Bi-directional integration}\cite{sw-testing-quality-assurance}\\
	This approach, also known as \textit{Build integration}\cite{schattenbest}, combines both Top-down and Bottom-up integration. 
	Individual components are grouped into categories. First components in each category are integrated bottom-up leaving the topmost layer.
	Then all remaining components are integrated top-down.
\end{itemize}
\paragraph{System Testing} ~\\
IEEE defines \textit{System testing} as\cite{ieee-definition} "testing conducted on a complete, integrated system to evaluate the system's compliance with its specified requirements". As the whole application is finished System testing begins. It aims as evaluating the system against initial requirements.
Depending on the type of requirement either test cases implementing \textit{functional} or \textit{non-functional} requirements are executed as they are defined during the requirement-gathering phase. 
\begin{itemize}
	\item \textbf{Functional testing}\\
	IEEE's defines Functional testing as \cite{ieee-definition}
	"Testing conducted to evaluate the compliance of a system or component with specified functional requirements". Thus functional tests ensure
	the correct functioning or behavior of the whole system and they are typically of type black-box.
	They focus on evaluating the outcome against predefined expectations. Functional testing of Web applications is hard due to the
	reasons mentioned in the introduction. This is the reason why there is no tool ultimate tool satisfying all needs and no best practice, though are many
	tools implementing different approaches.
	\item \textbf{Non-functional testing}\\
	There are several approaches implementing this kind of testing. One are performance tests. Since the well known publication of Gordon E. Moore back in 
	1965\cite{moore} which says that the number of integrated circuits will at least double every year computation power indeed highly increased.
	However performance tests are still important due to the increasing complexity and feature richness of todays applications. Another non-functional 
	testing approach is security testing. Though the Open Web Application Security Project, abbreviated OWASP\cite{owasp}, published the list of the top ten
	Web security threats, the number of exploits listed in the Metasploit Database\cite{metasploit} still rises. Thus security testing is crucial for the success
	of a software company. Apart from the above there are other approaches which would exceed the scope of this thesis. 
\end{itemize}
\section{Test Automation}
Most of the time developers spend time reading other's code. In the remaining time they write code, chat with colleagues or do other things.
Therefore time consuming log running tasks should be done manually by someone else or automated.
While the former might seem outdated in some areas there are still manual testing techniques which can not be replaced by automation.
One the other hand test automation tools gain increasing importance.\\
This section first compares the advantages and disadvantages of test automation and compares the different kinds of test automation frameworks.
\subsection{Pitfalls of Test Automation}
Possible pitfalls and misconceptions of test automation are listed below\cite{test-automation-success}.
\begin{itemize}
	\item \textbf{Automated testing completely replaces manual testing.}\\
	Often people who do not understand testing or even testers and test managers who should agree on this statement. Another related misconception 
	is to automate all tests. B. Marick\cite{testing-mistakes} differences 2 economic facts regarding automation versus manual testing:
	"If an automated test costs a certain amount of money, it will cost almost nothing to run from then on." and
	"If a manual test costs a certain amount of money to run the first time, it will cost just about the same as first time to run each time thereafter."
	These two statements conclude that complex automated testing should be done manually for economic reasons.
	However \cite{test-automation-success} lists some guidelines whether tests should be automated or not: Tests running on every build,
	testing on many different platforms and the same setup procedures for a lot of different tests. \\
	Another, yet more significant, reason is that the only way the evaluate automated tests by manual reviews. This is a variant of the well known
	chicken egg problem. In order to test the quality of automated tests meta test cases are required which again require additional test cases and so on.
	\item \textbf{The Pesticide Paradox}\\
	S. Desikan and G. Ramesh\cite{softare-testing-principles} compare writing software test cases with "designing the right pesticides to catch and kill the pests".
	It means that developers writing code get used to automated test cases and develop \textit{just for} passing the test but test cases never cover one hundred 
	percent of all possible execution paths. 
	\item \textbf{Independence}\\
	Probably every developer knows that Unit test should run independent from each other. Fortunately these tests usually have low coupling, requiring minimal
	overhead to set up. But \textit{all} kinds of tests should run in isolation, thus not affecting the outcome of other tests.
	This is extremely challenging especially for coarse grained tests like functional Web application tests. However some test cases logically depend on each other.
	We chose a simple Website offering online shopping functionality. One test case verifies the correct authentication behavior for an arbitrary user and
	another evaluates the correct computation of the total price if a customer adds items to the shopping cart.
	Clearly the latter is only possible after successful authentication, thus the result of the former test case determines the result of the latter.
	Nevertheless testing needs to done independently. Therefore we recommend an isolated test environment as close as possible the production environment
	as a best practice.
	\item \textbf{Repeatability}\\
	At least as important as the above aspect is if tests executed repeatedly produce the same outcome in a stable environment. One possible 
	indicator of different outcomes are misused threading. As threads can lead to unpredictable behavior they should be avoided where possible. 
	If this is undesirable synchronization techniques help to reduce the risk of deadlocks and race conditions. 
	Another cause may be misused random data. If some application requires initialization with random data like playing dice, either using constant data
	or pre-generating the random data helps to predict the outcome. 
\end{itemize}
Apart from the above challenges automated testing is a great technique to make testing explicit. If new developers write code they immediately recognize 
the requirements formulated in test cases. Of course this is not true if management does not acquire enough resources or it is not treated as development activity.
Testing is a full time job and it should be done by dedicated experts.
Another important aspect ensuring successful automated tests is documentation in case of a more complex testing environment. Useful might be for example
documenting steps necessary for running the test cases, manual test result review in case of partial automated tests and what part of the software is not
tested and why. 
\subsection{Test Automation Frameworks}
As the World Wide Web attracted more and more people there has been a growing need for automated evaluation of Web applications. Companies are willing to
invest a lot for a powerful automation tool. All the above approaches have the term \textit{framework} in common. R. Johnson and B. Foote define it
in the context of Object Oriented Programming as\cite{oop}
"a set of classes that embodies an abstract design for solutions to a family of related problems, and supports reuses at a larger granularity than classes".
This definition can also be applied to automated testing where the term \textit{Test Automation Framework} seems appropriate.\\
E. Kit\cite{kit} distinguishes three generations of automation tools:
\begin{enumerate}
	\item The most basic tools use the \textit{Capture and Playback} approach. It works as a one needs to manually perform all steps necessary for evaluation which
	are subsequently recorded for replaying, hence the word \textit{replay} is often used synonymously.
	These tools have one major drawback in common as high maintenance costs\cite{record-playback}. Once everything is recorded there is no guarantee that all
	test cases do not break in the future though all work now. Another yet more problematic disadvantage are hard-coded data in the tool generated
	scripting language. This should be avoided in any programming language\cite{automation-principles}. 
	The last problem during playback is synchronization issues due to the distributed nature of Web applications. Obviously it takes some time for the browser to
	fetch a Website. In order to function correctly the tool implementing capture and playback needs to properly guess the delay after the server has sent a Website.
	Of course no tool is able to predict the future thus (over-)estimating the delay or requiring the programmer to explicitly put synchronization points
	into the generated script. Both solutions are cumbersome and hard.
	\item Second generation Capture and Playback tools have lessons learned. Instead of a single capture phase tools require writing scripting code. Vendors 
	equip their tools with a full blown scripted language which brings all the advantages of a modern programming language like code-reuse, conditional execution and
	error recovery to name a few. However treating automation as a software engineering task requires additional domain experts having scripting knowledge.
	\item Tools from the above two generations generate a single script for all test cases. Third generation Capture and Playback tools use many small scripts each 
	responsible for a single test case. It further extracts hard-coded data out of the scripts leading to a new era of frameworks called
	\textit{Data-driven Frameworks}. 
\end{enumerate} 
\paragraph{Data-driven Frameworks} ~\\
The basic concept of Data-driven Automation Frameworks is a separation between the behavioral instructions and the data used as parameter for each test case. 
As \cite{record-playback} stated parameters are usually kept in separated files organized as a grid. There is no limitation about the actual data format though
for usability reasons using spreadsheet readable formats is recommended as xsl-files. Even though the concept of Data-driven Automation Frameworks is not application specific until today only tools testing GUI and Web applications can be found. In the remainder of this paragraph a popular Data-driven Automation
tool namely \textit{Selenium}\cite{selenium} specifically designed for Web application testing is presented.\\
Selenium was originally developed by Jason R. Huggins working on a new time tracking system for ThoughtWorks back in 2004\cite{shortcut-selenium}.
At that time client-side Javascript code was a nightmare to test due to the different browser implementations. Almost a decade later it is still one of 
the most challenging tasks. Basically Selenium includes two different open source tools: \textit{Selenium IDE} and \textit{Selenium 2}. 
The former provide testing novices the ability to get used to how tests are recorded and the scripting environment. It currently runs on Mozilla Firefox only thus
shipping as a Firefox extension. The recommended way of using Selenium is through the latter tool which is the successor of Selenium 1. Developers spent a lot of
effort in backward compatibility however is it of advantage to upgrade old Selenium 1 tests. \\\\
-TODO: further explanation and examples-\\\\
Due to the following major drawback the use of tools following a Data-driven approach is not recommended for commercial applications. The reason for this
is scalability. To understand this problem the internals of a tool following a Data-driven approach need to be examined. Each row in a data-grid represents exaclty 
a single test case. The implementation of the latter is done either automatically by recording user interactions or specified manually.
In either way as more applications code needs to be tested new test cases generated in one of the above ways are required. 
This leads to the conclusion that the number of lines of automation code is \textit{proportional} to the number of tests\cite{record-playback} which is generally 
undesirable. 
\paragraph{Model-driven Frameworks} ~\\
Text...
\paragraph{Other Frameworks} ~\\
Text...

\section{Resilience and Adaptiveness}
One of the main challenges in Web application testing are changing Webpages. There are even categories of Websites which were build with the intention of regular
changes such as News sites. Whereas on the fronted side news seem to be updates in real time, news sites are usually realized using a Content Management System. 
CMS systems ease the development of high adaptive Web applications, though requiring more sophisticated testing approaches. This section gives a brief overview 
of different approaches satisfying this requirement. 
\subsection{Robust XPath}

\subsection{Maximum Tree Matching}

\subsection{Other Approaches}

\newpage
\bibliography{literature}
\bibliographystyle{plain}
\end{document}


